{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAINING & EVALUATION\n",
    "\n",
    "Let us train some models!\n",
    "\n",
    "For reference the list of models and training data are found below:\n",
    "\n",
    "#### SIMPLE MODELS\n",
    "1) tf-idf + Logistic Regression\n",
    "2) tf-idf + Multinomial Naive Bayes\n",
    "3) tf-idf + Decision Tree\n",
    "4) tf-idf + Random Forest\n",
    "5) tf-idf + kNN\n",
    "6) DistilBERT embeddings + Logistic Regression\n",
    "7) DistilBERT embeddings + Multinomial Naive Bayes\n",
    "8) DistilBERT embeddings + Decision Tree\n",
    "9) DistilBERT embeddings + Random Forest\n",
    "10) DistilBERT embeddings + kNN\n",
    "\n",
    "#### TRANSFORMER MODELS\n",
    "1) DistilBERT\n",
    "\n",
    "#### TRAINING DATA\n",
    "1) Cleaned training data\n",
    "2) Cleaned training data with topic information\n",
    "3) Cleaned training data with topic information and with balanced sentiment representation via over- and undersampling.\n",
    "\n",
    "We will evaluate using precision, recall and accuracy scores, which are the standard metrics for this task. For each model we will be saving the best performing one based on their training data composition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's implement the methods for training and evaluation. Also implement a method to save the models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (DatabaseError('database disk image is malformed')).History will not be written to the database.\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA \n",
    "import joblib\n",
    "import os\n",
    "\n",
    "\n",
    "def train_model(X_train, y_train, X_test, y_test, vectorizer, classifier, use_pca=False, n_components=100, log_reg_max_iter=1000):\n",
    "    started = time()\n",
    "\n",
    "    if classifier == \"LogisticRegression\":\n",
    "        from sklearn.linear_model import LogisticRegression\n",
    "        classifier = LogisticRegression(max_iter=log_reg_max_iter)\n",
    "\n",
    "    if classifier == \"MultinomialNB\":\n",
    "        from sklearn.naive_bayes import MultinomialNB\n",
    "        classifier = MultinomialNB()\n",
    "        if vectorizer is None:\n",
    "            X_train = MinMaxScaler().fit_transform(X_train)\n",
    "            X_test = MinMaxScaler().fit_transform(X_test)\n",
    "\n",
    "    if classifier == \"DecisionTreeClassifier\":\n",
    "        from sklearn.tree import DecisionTreeClassifier\n",
    "        classifier = DecisionTreeClassifier()\n",
    "    \n",
    "    if classifier == \"RandomForestClassifier\":\n",
    "        from sklearn.ensemble import RandomForestClassifier\n",
    "        classifier = RandomForestClassifier()\n",
    "\n",
    "    if classifier == \"KNeighborsClassifier\":\n",
    "        from sklearn.neighbors import KNeighborsClassifier\n",
    "        classifier = KNeighborsClassifier()\n",
    "    \n",
    "\n",
    "    steps = []\n",
    "    if vectorizer is not None:\n",
    "        steps.append(('vectorizer', vectorizer))\n",
    "    if use_pca:\n",
    "        # Add PCA to the pipeline if requested\n",
    "        steps.append(('pca', PCA(n_components=n_components)))\n",
    "        # steps.append(('scaler', MinMaxScaler()))\n",
    "    steps.append(('classifier', classifier))\n",
    "\n",
    "    # Construct the pipeline from the specified steps\n",
    "    pipeline = make_pipeline(*[step for _, step in steps])\n",
    "\n",
    "\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    finished = time()\n",
    "\n",
    "    print(f'Training time: {finished - started:.2f}s')\n",
    "\n",
    "\n",
    "    return pipeline\n",
    "\n",
    "def evaluate_model(pipeline, X_test, y_test):\n",
    "\n",
    "    started = time()\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    finished = time()\n",
    "    print(f'Prediction time: {finished - started:.2f}s')\n",
    "\n",
    "def save_model(pipeline, model_path):\n",
    "    if not os.path.exists(os.path.dirname(model_path)):\n",
    "        os.makedirs(os.path.dirname(model_path))\n",
    "    joblib.dump(pipeline, model_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SIMPLE MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets load the data\n",
    "import pandas as pd\n",
    "\n",
    "training_df_cleaned = pd.read_csv('data/training_cleaned.csv')\n",
    "validation_df_cleaned = pd.read_csv('data/validation_cleaned.csv')\n",
    "\n",
    "training_df_topic_merged = pd.read_csv('data/training_topic_merged.csv')\n",
    "validation_df_topic_merged = pd.read_csv('data/validation_topic_merged.csv')\n",
    "\n",
    "training_df_balanced_us = pd.read_csv('data/training_balanced_us.csv')\n",
    "# validation dfs for balanced datasets will be validation_df_topic_merged\n",
    "\n",
    "training_df_balanced_os = pd.read_csv('data/training_balanced_os.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the models will only get the tweet or the tweet_topic as input, so lets make the partitions\n",
    "\n",
    "# 1. Cleaned data\n",
    "X_train_cleaned, y_train_cleaned = training_df_cleaned['tweet'], training_df_cleaned['sentiment']\n",
    "X_test_cleaned, y_test_cleaned = validation_df_cleaned['tweet'], validation_df_cleaned['sentiment']\n",
    "\n",
    "# 2. Topic merged data\n",
    "X_train_topic_merged, y_train_topic_merged = training_df_topic_merged['topic_tweet'], training_df_topic_merged['sentiment']\n",
    "X_test_topic_merged, y_test_topic_merged = validation_df_topic_merged['topic_tweet'], validation_df_topic_merged['sentiment']\n",
    "\n",
    "# 3. Balanced undersampled data\n",
    "X_train_balanced_us, y_train_balanced_us = training_df_balanced_us['topic_tweet'], training_df_balanced_us['sentiment']\n",
    "X_test_balanced_us, y_test_balanced_us = validation_df_topic_merged['topic_tweet'], validation_df_topic_merged['sentiment']\n",
    "\n",
    "# 4. Balanced oversampled data\n",
    "X_train_balanced_os, y_train_balanced_os = training_df_balanced_os['topic_tweet'], training_df_balanced_os['sentiment']\n",
    "X_test_balanced_os, y_test_balanced_os = validation_df_topic_merged['topic_tweet'], validation_df_topic_merged['sentiment']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF + LOGISTIC REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "cleaned_vectorizer = TfidfVectorizer()\n",
    "topic_merged_vectorizer = TfidfVectorizer()\n",
    "balanced_us_vectorizer = TfidfVectorizer()\n",
    "balanced_os_vectorizer = TfidfVectorizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 103.96s\n",
      "Training time: 138.90s\n",
      "Training time: 39.10s\n",
      "Training time: 144.90s\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Irrelevant       0.88      0.85      0.86       172\n",
      "    Negative       0.86      0.94      0.89       266\n",
      "     Neutral       0.94      0.85      0.89       285\n",
      "    Positive       0.90      0.93      0.91       277\n",
      "\n",
      "    accuracy                           0.89      1000\n",
      "   macro avg       0.89      0.89      0.89      1000\n",
      "weighted avg       0.90      0.89      0.89      1000\n",
      "\n",
      "Prediction time: 0.19s\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Irrelevant       0.87      0.83      0.85       172\n",
      "    Negative       0.85      0.93      0.89       266\n",
      "     Neutral       0.95      0.87      0.90       285\n",
      "    Positive       0.88      0.91      0.90       277\n",
      "\n",
      "    accuracy                           0.89      1000\n",
      "   macro avg       0.89      0.88      0.88      1000\n",
      "weighted avg       0.89      0.89      0.89      1000\n",
      "\n",
      "Prediction time: 0.22s\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Irrelevant       0.65      0.76      0.70       172\n",
      "    Negative       0.76      0.79      0.78       266\n",
      "     Neutral       0.82      0.73      0.77       285\n",
      "    Positive       0.82      0.80      0.81       277\n",
      "\n",
      "    accuracy                           0.77      1000\n",
      "   macro avg       0.76      0.77      0.76      1000\n",
      "weighted avg       0.77      0.77      0.77      1000\n",
      "\n",
      "Prediction time: 0.17s\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Irrelevant       0.78      0.82      0.80       172\n",
      "    Negative       0.85      0.89      0.87       266\n",
      "     Neutral       0.91      0.85      0.88       285\n",
      "    Positive       0.89      0.89      0.89       277\n",
      "\n",
      "    accuracy                           0.87      1000\n",
      "   macro avg       0.86      0.86      0.86      1000\n",
      "weighted avg       0.87      0.87      0.87      1000\n",
      "\n",
      "Prediction time: 0.17s\n"
     ]
    }
   ],
   "source": [
    "cleaned_log_reg_model = train_model(X_train_cleaned, y_train_cleaned, X_test_cleaned, y_test_cleaned, cleaned_vectorizer, 'LogisticRegression')\n",
    "topic_merged_log_reg_model = train_model(X_train_topic_merged, y_train_topic_merged, X_test_topic_merged, y_test_topic_merged, topic_merged_vectorizer, 'LogisticRegression')\n",
    "balanced_us_log_reg_model = train_model(X_train_balanced_us, y_train_balanced_us, X_test_balanced_us, y_test_balanced_us, balanced_us_vectorizer, 'LogisticRegression')\n",
    "balanced_os_log_reg_model = train_model(X_train_balanced_os, y_train_balanced_os, X_test_balanced_os, y_test_balanced_os, balanced_os_vectorizer, 'LogisticRegression')\n",
    "\n",
    "evaluate_model(cleaned_log_reg_model, X_test_cleaned, y_test_cleaned)\n",
    "evaluate_model(topic_merged_log_reg_model, X_test_topic_merged, y_test_topic_merged)\n",
    "evaluate_model(balanced_us_log_reg_model, X_test_balanced_us, y_test_balanced_us)\n",
    "evaluate_model(balanced_os_log_reg_model, X_test_balanced_os, y_test_balanced_os)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BEST PERFORMANCE (Accuracy) = Cleaned with score 0.89"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets save the best model\n",
    "save_model(cleaned_log_reg_model, 'models/simple/tfidf/cleaned_log_reg_model.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF + MULTINOMIAL NAIVE BAYES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 4.92s\n",
      "Training time: 4.76s\n",
      "Training time: 1.94s\n",
      "Training time: 7.95s\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Irrelevant       0.95      0.53      0.68       172\n",
      "    Negative       0.66      0.93      0.77       266\n",
      "     Neutral       0.91      0.64      0.75       285\n",
      "    Positive       0.74      0.88      0.80       277\n",
      "\n",
      "    accuracy                           0.77      1000\n",
      "   macro avg       0.82      0.75      0.75      1000\n",
      "weighted avg       0.80      0.77      0.76      1000\n",
      "\n",
      "Prediction time: 0.11s\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Irrelevant       0.95      0.53      0.68       172\n",
      "    Negative       0.68      0.93      0.78       266\n",
      "     Neutral       0.91      0.64      0.75       285\n",
      "    Positive       0.73      0.89      0.80       277\n",
      "\n",
      "    accuracy                           0.77      1000\n",
      "   macro avg       0.82      0.75      0.75      1000\n",
      "weighted avg       0.80      0.77      0.76      1000\n",
      "\n",
      "Prediction time: 0.14s\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Irrelevant       0.66      0.72      0.69       172\n",
      "    Negative       0.69      0.85      0.76       266\n",
      "     Neutral       0.85      0.64      0.73       285\n",
      "    Positive       0.79      0.78      0.79       277\n",
      "\n",
      "    accuracy                           0.75      1000\n",
      "   macro avg       0.75      0.74      0.74      1000\n",
      "weighted avg       0.76      0.75      0.75      1000\n",
      "\n",
      "Prediction time: 0.19s\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Irrelevant       0.78      0.72      0.75       172\n",
      "    Negative       0.75      0.89      0.82       266\n",
      "     Neutral       0.90      0.73      0.81       285\n",
      "    Positive       0.80      0.84      0.82       277\n",
      "\n",
      "    accuracy                           0.81      1000\n",
      "   macro avg       0.81      0.80      0.80      1000\n",
      "weighted avg       0.81      0.81      0.80      1000\n",
      "\n",
      "Prediction time: 0.18s\n"
     ]
    }
   ],
   "source": [
    "cleaned_mnb_model = train_model(X_train_cleaned, y_train_cleaned, X_test_cleaned, y_test_cleaned, cleaned_vectorizer, 'MultinomialNB')\n",
    "topic_merged_mnb_model = train_model(X_train_topic_merged, y_train_topic_merged, X_test_topic_merged, y_test_topic_merged, topic_merged_vectorizer, 'MultinomialNB')\n",
    "balanced_us_mnb_model = train_model(X_train_balanced_us, y_train_balanced_us, X_test_balanced_us, y_test_balanced_us, balanced_us_vectorizer, 'MultinomialNB')\n",
    "balanced_os_mnb_model = train_model(X_train_balanced_os, y_train_balanced_os, X_test_balanced_os, y_test_balanced_os, balanced_os_vectorizer, 'MultinomialNB')\n",
    "\n",
    "evaluate_model(cleaned_mnb_model, X_test_cleaned, y_test_cleaned)\n",
    "evaluate_model(topic_merged_mnb_model, X_test_topic_merged, y_test_topic_merged)\n",
    "evaluate_model(balanced_us_mnb_model, X_test_balanced_us, y_test_balanced_us)\n",
    "evaluate_model(balanced_os_mnb_model, X_test_balanced_os, y_test_balanced_os)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BEST PERFORMANCE (Accuracy) = Balanced w/ Oversample with score 0.81\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets save the best model\n",
    "save_model(balanced_os_mnb_model, 'models/simple/tfidf/balanced_os_mnb_model.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF + DECISION TREE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 132.02s\n",
      "Training time: 105.73s\n",
      "Training time: 87.60s\n",
      "Training time: 297.81s\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Irrelevant       0.93      0.91      0.92       172\n",
      "    Negative       0.93      0.96      0.94       266\n",
      "     Neutral       0.93      0.91      0.92       285\n",
      "    Positive       0.92      0.92      0.92       277\n",
      "\n",
      "    accuracy                           0.93      1000\n",
      "   macro avg       0.93      0.93      0.93      1000\n",
      "weighted avg       0.93      0.93      0.93      1000\n",
      "\n",
      "Prediction time: 0.32s\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Irrelevant       0.92      0.92      0.92       172\n",
      "    Negative       0.92      0.95      0.93       266\n",
      "     Neutral       0.94      0.88      0.91       285\n",
      "    Positive       0.92      0.94      0.93       277\n",
      "\n",
      "    accuracy                           0.92      1000\n",
      "   macro avg       0.92      0.92      0.92      1000\n",
      "weighted avg       0.92      0.92      0.92      1000\n",
      "\n",
      "Prediction time: 0.66s\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Irrelevant       0.71      0.84      0.77       172\n",
      "    Negative       0.81      0.75      0.78       266\n",
      "     Neutral       0.79      0.72      0.75       285\n",
      "    Positive       0.73      0.76      0.74       277\n",
      "\n",
      "    accuracy                           0.76      1000\n",
      "   macro avg       0.76      0.77      0.76      1000\n",
      "weighted avg       0.76      0.76      0.76      1000\n",
      "\n",
      "Prediction time: 0.27s\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Irrelevant       0.92      0.89      0.90       172\n",
      "    Negative       0.89      0.94      0.91       266\n",
      "     Neutral       0.89      0.87      0.88       285\n",
      "    Positive       0.91      0.91      0.91       277\n",
      "\n",
      "    accuracy                           0.90      1000\n",
      "   macro avg       0.90      0.90      0.90      1000\n",
      "weighted avg       0.90      0.90      0.90      1000\n",
      "\n",
      "Prediction time: 0.33s\n"
     ]
    }
   ],
   "source": [
    "cleaned_dt_model = train_model(X_train_cleaned, y_train_cleaned, X_test_cleaned, y_test_cleaned, cleaned_vectorizer, 'DecisionTreeClassifier')\n",
    "topic_merged_dt_model = train_model(X_train_topic_merged, y_train_topic_merged, X_test_topic_merged, y_test_topic_merged, topic_merged_vectorizer, 'DecisionTreeClassifier')\n",
    "balanced_us_dt_model = train_model(X_train_balanced_us, y_train_balanced_us, X_test_balanced_us, y_test_balanced_us, balanced_us_vectorizer, 'DecisionTreeClassifier')\n",
    "balanced_os_dt_model = train_model(X_train_balanced_os, y_train_balanced_os, X_test_balanced_os, y_test_balanced_os, balanced_os_vectorizer, 'DecisionTreeClassifier')\n",
    "\n",
    "evaluate_model(cleaned_dt_model, X_test_cleaned, y_test_cleaned)\n",
    "evaluate_model(topic_merged_dt_model, X_test_topic_merged, y_test_topic_merged)\n",
    "evaluate_model(balanced_us_dt_model, X_test_balanced_us, y_test_balanced_us)\n",
    "evaluate_model(balanced_os_dt_model, X_test_balanced_os, y_test_balanced_os)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BEST PERFORMANCE (Accuracy) = Cleaned with score 0.93"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets save the best model\n",
    "save_model(cleaned_dt_model, 'models/simple/tfidf/cleaned_dt_model.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF + RANDOM FOREST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 2244.70s\n",
      "Training time: 1724.63s\n",
      "Training time: 401.65s\n",
      "Training time: 2057.45s\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Irrelevant       0.99      0.97      0.98       172\n",
      "    Negative       0.97      0.98      0.98       266\n",
      "     Neutral       0.97      0.97      0.97       285\n",
      "    Positive       0.97      0.97      0.97       277\n",
      "\n",
      "    accuracy                           0.97      1000\n",
      "   macro avg       0.97      0.97      0.97      1000\n",
      "weighted avg       0.97      0.97      0.97      1000\n",
      "\n",
      "Prediction time: 7.01s\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Irrelevant       0.98      0.99      0.99       172\n",
      "    Negative       0.96      0.98      0.97       266\n",
      "     Neutral       0.99      0.97      0.98       285\n",
      "    Positive       0.98      0.98      0.98       277\n",
      "\n",
      "    accuracy                           0.98      1000\n",
      "   macro avg       0.98      0.98      0.98      1000\n",
      "weighted avg       0.98      0.98      0.98      1000\n",
      "\n",
      "Prediction time: 5.06s\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Irrelevant       0.82      0.92      0.87       172\n",
      "    Negative       0.87      0.88      0.88       266\n",
      "     Neutral       0.93      0.82      0.87       285\n",
      "    Positive       0.88      0.90      0.89       277\n",
      "\n",
      "    accuracy                           0.88      1000\n",
      "   macro avg       0.87      0.88      0.88      1000\n",
      "weighted avg       0.88      0.88      0.88      1000\n",
      "\n",
      "Prediction time: 0.62s\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Irrelevant       0.99      0.96      0.97       172\n",
      "    Negative       0.95      0.98      0.96       266\n",
      "     Neutral       0.97      0.95      0.96       285\n",
      "    Positive       0.97      0.98      0.97       277\n",
      "\n",
      "    accuracy                           0.97      1000\n",
      "   macro avg       0.97      0.97      0.97      1000\n",
      "weighted avg       0.97      0.97      0.97      1000\n",
      "\n",
      "Prediction time: 0.78s\n"
     ]
    }
   ],
   "source": [
    "cleaned_rf_model = train_model(X_train_cleaned, y_train_cleaned, X_test_cleaned, y_test_cleaned, cleaned_vectorizer, 'RandomForestClassifier')\n",
    "topic_merged_rf_model = train_model(X_train_topic_merged, y_train_topic_merged, X_test_topic_merged, y_test_topic_merged, topic_merged_vectorizer, 'RandomForestClassifier')\n",
    "balanced_us_rf_model = train_model(X_train_balanced_us, y_train_balanced_us, X_test_balanced_us, y_test_balanced_us, balanced_us_vectorizer, 'RandomForestClassifier')\n",
    "balanced_os_rf_model = train_model(X_train_balanced_os, y_train_balanced_os, X_test_balanced_os, y_test_balanced_os, balanced_os_vectorizer, 'RandomForestClassifier')\n",
    "\n",
    "evaluate_model(cleaned_rf_model, X_test_cleaned, y_test_cleaned)\n",
    "evaluate_model(topic_merged_rf_model, X_test_topic_merged, y_test_topic_merged)\n",
    "evaluate_model(balanced_us_rf_model, X_test_balanced_us, y_test_balanced_us)\n",
    "evaluate_model(balanced_os_rf_model, X_test_balanced_os, y_test_balanced_os)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BEST PERFORMANCE (Accuracy) = Topic Merged with score 0.98"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets save the best model\n",
    "save_model(topic_merged_rf_model, 'models/simple/tfidf/topic_merged_rf_model.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF + kNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 6.55s\n",
      "Training time: 7.40s\n",
      "Training time: 3.06s\n",
      "Training time: 10.88s\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Irrelevant       0.93      0.98      0.95       172\n",
      "    Negative       0.95      0.97      0.96       266\n",
      "     Neutral       0.95      0.96      0.96       285\n",
      "    Positive       0.98      0.92      0.95       277\n",
      "\n",
      "    accuracy                           0.95      1000\n",
      "   macro avg       0.95      0.96      0.95      1000\n",
      "weighted avg       0.95      0.95      0.95      1000\n",
      "\n",
      "Prediction time: 6.21s\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Irrelevant       0.96      0.99      0.98       172\n",
      "    Negative       0.98      0.98      0.98       266\n",
      "     Neutral       0.99      0.97      0.98       285\n",
      "    Positive       0.99      0.98      0.98       277\n",
      "\n",
      "    accuracy                           0.98      1000\n",
      "   macro avg       0.98      0.98      0.98      1000\n",
      "weighted avg       0.98      0.98      0.98      1000\n",
      "\n",
      "Prediction time: 5.67s\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Irrelevant       0.70      0.79      0.74       172\n",
      "    Negative       0.71      0.73      0.72       266\n",
      "     Neutral       0.79      0.71      0.74       285\n",
      "    Positive       0.75      0.74      0.75       277\n",
      "\n",
      "    accuracy                           0.74      1000\n",
      "   macro avg       0.74      0.74      0.74      1000\n",
      "weighted avg       0.74      0.74      0.74      1000\n",
      "\n",
      "Prediction time: 2.79s\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Irrelevant       0.97      0.98      0.97       172\n",
      "    Negative       0.97      0.96      0.96       266\n",
      "     Neutral       0.96      0.96      0.96       285\n",
      "    Positive       0.95      0.96      0.95       277\n",
      "\n",
      "    accuracy                           0.96      1000\n",
      "   macro avg       0.96      0.96      0.96      1000\n",
      "weighted avg       0.96      0.96      0.96      1000\n",
      "\n",
      "Prediction time: 32.66s\n"
     ]
    }
   ],
   "source": [
    "cleaned_knn_model = train_model(X_train_cleaned, y_train_cleaned, X_test_cleaned, y_test_cleaned, cleaned_vectorizer, 'KNeighborsClassifier')\n",
    "topic_merged_knn_model = train_model(X_train_topic_merged, y_train_topic_merged, X_test_topic_merged, y_test_topic_merged, topic_merged_vectorizer, 'KNeighborsClassifier')\n",
    "balanced_us_knn_model = train_model(X_train_balanced_us, y_train_balanced_us, X_test_balanced_us, y_test_balanced_us, balanced_us_vectorizer, 'KNeighborsClassifier')\n",
    "balanced_os_knn_model = train_model(X_train_balanced_os, y_train_balanced_os, X_test_balanced_os, y_test_balanced_os, balanced_os_vectorizer, 'KNeighborsClassifier')\n",
    "\n",
    "evaluate_model(cleaned_knn_model, X_test_cleaned, y_test_cleaned)\n",
    "evaluate_model(topic_merged_knn_model, X_test_topic_merged, y_test_topic_merged)\n",
    "evaluate_model(balanced_us_knn_model, X_test_balanced_us, y_test_balanced_us)\n",
    "evaluate_model(balanced_os_knn_model, X_test_balanced_os, y_test_balanced_os)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BEST PERFORMANCE (Accuracy) = Topic Merged with score 0.98\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets save the best model\n",
    "save_model(topic_merged_knn_model, 'models/simple/tfidf/topic_merged_knn_model.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DistilBERT Embeddings + Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets load the data\n",
    "import torch\n",
    "\n",
    "X_train_cleaned_bert = torch.load('data/X_train_cleaned_bert.pt')\n",
    "X_test_cleaned_bert = torch.load('data/X_test_cleaned_bert.pt')\n",
    "\n",
    "X_train_topic_merged_bert = torch.load('data/X_train_topic_merged_bert.pt')\n",
    "X_test_topic_merged_bert = torch.load('data/X_test_topic_merged_bert.pt')\n",
    "\n",
    "X_train_balanced_us_bert = torch.load('data/X_train_balanced_us_bert.pt')\n",
    "X_test_balanced_us_bert = X_test_topic_merged_bert\n",
    "\n",
    "X_train_balanced_os_bert = torch.load('data/X_train_balanced_os_bert.pt')\n",
    "X_test_balanced_os_bert = X_test_topic_merged_bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\envs\\huk-cc\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 519.73s\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Irrelevant       0.60      0.38      0.46       172\n",
      "    Negative       0.57      0.75      0.65       266\n",
      "     Neutral       0.61      0.52      0.56       285\n",
      "    Positive       0.63      0.69      0.66       277\n",
      "\n",
      "    accuracy                           0.60      1000\n",
      "   macro avg       0.60      0.58      0.58      1000\n",
      "weighted avg       0.61      0.60      0.59      1000\n",
      "\n",
      "Prediction time: 0.10s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\envs\\huk-cc\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 533.97s\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Irrelevant       0.58      0.40      0.48       172\n",
      "    Negative       0.61      0.75      0.67       266\n",
      "     Neutral       0.65      0.57      0.61       285\n",
      "    Positive       0.63      0.69      0.66       277\n",
      "\n",
      "    accuracy                           0.62      1000\n",
      "   macro avg       0.62      0.60      0.60      1000\n",
      "weighted avg       0.62      0.62      0.62      1000\n",
      "\n",
      "Prediction time: 0.10s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[60], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m topic_merged_lr_bertembed_model, _, _ \u001b[38;5;241m=\u001b[39m train_model(X_train_topic_merged_bert, y_train_topic_merged, X_test_topic_merged_bert, y_test_topic_merged, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLogisticRegression\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      5\u001b[0m evaluate_model(topic_merged_lr_bertembed_model, X_test_topic_merged_bert, y_test_topic_merged)\n\u001b[1;32m----> 7\u001b[0m balanced_us_lr_bertembed_model, _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_balanced_us_bert\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_balanced_us\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test_balanced_us_bert\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test_balanced_us\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mLogisticRegression\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m evaluate_model(balanced_us_lr_bertembed_model, X_test_balanced_us_bert, y_test_balanced_us)\n\u001b[0;32m     10\u001b[0m balanced_os_lr_bertembed_model, _, _ \u001b[38;5;241m=\u001b[39m train_model(X_train_balanced_os_bert, y_train_balanced_os, X_test_balanced_os_bert, y_test_balanced_os, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLogisticRegression\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[43], line 50\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(X_train, y_train, X_test, y_test, vectorizer, classifier, use_pca, n_components)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# Construct the pipeline from the specified steps\u001b[39;00m\n\u001b[0;32m     47\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m make_pipeline(\u001b[38;5;241m*\u001b[39m[step \u001b[38;5;28;01mfor\u001b[39;00m _, step \u001b[38;5;129;01min\u001b[39;00m steps])\n\u001b[1;32m---> 50\u001b[0m \u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     52\u001b[0m finished \u001b[38;5;241m=\u001b[39m time()\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining time: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfinished\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mstarted\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\huk-cc\\Lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\huk-cc\\Lib\\site-packages\\sklearn\\pipeline.py:473\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m    471\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    472\u001b[0m         last_step_params \u001b[38;5;241m=\u001b[39m routed_params[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m]]\n\u001b[1;32m--> 473\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_final_estimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mlast_step_params\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfit\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    475\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\huk-cc\\Lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\huk-cc\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1350\u001b[0m, in \u001b[0;36mLogisticRegression.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1347\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1348\u001b[0m     n_threads \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 1350\u001b[0m fold_coefs_ \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprefer\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1352\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1353\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1354\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpos_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1355\u001b[0m \u001b[43m        \u001b[49m\u001b[43mCs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mC_\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1356\u001b[0m \u001b[43m        \u001b[49m\u001b[43ml1_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43ml1_ratio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1357\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfit_intercept\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_intercept\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1358\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1359\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1360\u001b[0m \u001b[43m        \u001b[49m\u001b[43msolver\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msolver\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1361\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmulti_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmulti_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1362\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1363\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1364\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1365\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1366\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcoef\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwarm_start_coef_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1367\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpenalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpenalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1368\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_squared_sum\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_squared_sum\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1369\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1370\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_threads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_threads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1371\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1372\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mclass_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarm_start_coef_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mclasses_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarm_start_coef\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1373\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1375\u001b[0m fold_coefs_, _, n_iter_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mfold_coefs_)\n\u001b[0;32m   1376\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_iter_ \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(n_iter_, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mint32)[:, \u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\huk-cc\\Lib\\site-packages\\sklearn\\utils\\parallel.py:74\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     69\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     70\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     71\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     73\u001b[0m )\n\u001b[1;32m---> 74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\huk-cc\\Lib\\site-packages\\joblib\\parallel.py:1918\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1916\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[0;32m   1917\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 1918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1920\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[0;32m   1921\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[0;32m   1922\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[0;32m   1923\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[0;32m   1924\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[0;32m   1925\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\huk-cc\\Lib\\site-packages\\joblib\\parallel.py:1847\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1845\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1846\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 1847\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1848\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1849\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\huk-cc\\Lib\\site-packages\\sklearn\\utils\\parallel.py:136\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    134\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[1;32m--> 136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\huk-cc\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:455\u001b[0m, in \u001b[0;36m_logistic_regression_path\u001b[1;34m(X, y, pos_class, Cs, fit_intercept, max_iter, tol, verbose, solver, coef, class_weight, dual, penalty, intercept_scaling, multi_class, random_state, check_input, max_squared_sum, sample_weight, l1_ratio, n_threads)\u001b[0m\n\u001b[0;32m    451\u001b[0m l2_reg_strength \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m/\u001b[39m (C \u001b[38;5;241m*\u001b[39m sw_sum)\n\u001b[0;32m    452\u001b[0m iprint \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m50\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m101\u001b[39m][\n\u001b[0;32m    453\u001b[0m     np\u001b[38;5;241m.\u001b[39msearchsorted(np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m]), verbose)\n\u001b[0;32m    454\u001b[0m ]\n\u001b[1;32m--> 455\u001b[0m opt_res \u001b[38;5;241m=\u001b[39m \u001b[43moptimize\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mminimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    456\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    457\u001b[0m \u001b[43m    \u001b[49m\u001b[43mw0\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    458\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mL-BFGS-B\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    459\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjac\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    460\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ml2_reg_strength\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_threads\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    461\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[0;32m    462\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaxiter\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    463\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaxls\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# default is 20\u001b[39;49;00m\n\u001b[0;32m    464\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43miprint\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43miprint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    465\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgtol\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    466\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mftol\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfinfo\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    467\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    468\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    469\u001b[0m n_iter_i \u001b[38;5;241m=\u001b[39m _check_optimize_result(\n\u001b[0;32m    470\u001b[0m     solver,\n\u001b[0;32m    471\u001b[0m     opt_res,\n\u001b[0;32m    472\u001b[0m     max_iter,\n\u001b[0;32m    473\u001b[0m     extra_warning_msg\u001b[38;5;241m=\u001b[39m_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n\u001b[0;32m    474\u001b[0m )\n\u001b[0;32m    475\u001b[0m w0, loss \u001b[38;5;241m=\u001b[39m opt_res\u001b[38;5;241m.\u001b[39mx, opt_res\u001b[38;5;241m.\u001b[39mfun\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\huk-cc\\Lib\\site-packages\\scipy\\optimize\\_minimize.py:731\u001b[0m, in \u001b[0;36mminimize\u001b[1;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[0;32m    728\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_newtoncg(fun, x0, args, jac, hess, hessp, callback,\n\u001b[0;32m    729\u001b[0m                              \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[0;32m    730\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m meth \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ml-bfgs-b\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m--> 731\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43m_minimize_lbfgsb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjac\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    732\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    733\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m meth \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtnc\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    734\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_tnc(fun, x0, args, jac, bounds, callback\u001b[38;5;241m=\u001b[39mcallback,\n\u001b[0;32m    735\u001b[0m                         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\huk-cc\\Lib\\site-packages\\scipy\\optimize\\_lbfgsb_py.py:407\u001b[0m, in \u001b[0;36m_minimize_lbfgsb\u001b[1;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, finite_diff_rel_step, **unknown_options)\u001b[0m\n\u001b[0;32m    401\u001b[0m task_str \u001b[38;5;241m=\u001b[39m task\u001b[38;5;241m.\u001b[39mtobytes()\n\u001b[0;32m    402\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m task_str\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFG\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m    403\u001b[0m     \u001b[38;5;66;03m# The minimization routine wants f and g at the current x.\u001b[39;00m\n\u001b[0;32m    404\u001b[0m     \u001b[38;5;66;03m# Note that interruptions due to maxfun are postponed\u001b[39;00m\n\u001b[0;32m    405\u001b[0m     \u001b[38;5;66;03m# until the completion of the current minimization iteration.\u001b[39;00m\n\u001b[0;32m    406\u001b[0m     \u001b[38;5;66;03m# Overwrite f and g:\u001b[39;00m\n\u001b[1;32m--> 407\u001b[0m     f, g \u001b[38;5;241m=\u001b[39m \u001b[43mfunc_and_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    408\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m task_str\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNEW_X\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m    409\u001b[0m     \u001b[38;5;66;03m# new iteration\u001b[39;00m\n\u001b[0;32m    410\u001b[0m     n_iterations \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\huk-cc\\Lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:343\u001b[0m, in \u001b[0;36mScalarFunction.fun_and_grad\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    341\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray_equal(x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx):\n\u001b[0;32m    342\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_x(x)\n\u001b[1;32m--> 343\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_fun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    344\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_grad()\n\u001b[0;32m    345\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mg\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\huk-cc\\Lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:294\u001b[0m, in \u001b[0;36mScalarFunction._update_fun\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    292\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_update_fun\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    293\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_updated:\n\u001b[1;32m--> 294\u001b[0m         fx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wrapped_fun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    295\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m fx \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lowest_f:\n\u001b[0;32m    296\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lowest_x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\huk-cc\\Lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:20\u001b[0m, in \u001b[0;36m_wrapper_fun.<locals>.wrapped\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     16\u001b[0m ncalls[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Send a copy because the user may overwrite it.\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Overwriting results in undefined behaviour because\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# fun(self.x) will change self.x, with the two no longer linked.\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m fx \u001b[38;5;241m=\u001b[39m \u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Make sure the function returns a true scalar\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39misscalar(fx):\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\huk-cc\\Lib\\site-packages\\scipy\\optimize\\_optimize.py:79\u001b[0m, in \u001b[0;36mMemoizeJac.__call__\u001b[1;34m(self, x, *args)\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, \u001b[38;5;241m*\u001b[39margs):\n\u001b[0;32m     78\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\" returns the function value \"\"\"\u001b[39;00m\n\u001b[1;32m---> 79\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_compute_if_needed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\huk-cc\\Lib\\site-packages\\scipy\\optimize\\_optimize.py:73\u001b[0m, in \u001b[0;36mMemoizeJac._compute_if_needed\u001b[1;34m(self, x, *args)\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39mall(x \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjac \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(x)\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m---> 73\u001b[0m     fg \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     74\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjac \u001b[38;5;241m=\u001b[39m fg[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value \u001b[38;5;241m=\u001b[39m fg[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\huk-cc\\Lib\\site-packages\\sklearn\\linear_model\\_linear_loss.py:303\u001b[0m, in \u001b[0;36mLinearModelLoss.loss_gradient\u001b[1;34m(self, coef, X, y, sample_weight, l2_reg_strength, n_threads, raw_prediction)\u001b[0m\n\u001b[0;32m    301\u001b[0m grad[:, :n_features] \u001b[38;5;241m=\u001b[39m grad_pointwise\u001b[38;5;241m.\u001b[39mT \u001b[38;5;241m@\u001b[39m X \u001b[38;5;241m+\u001b[39m l2_reg_strength \u001b[38;5;241m*\u001b[39m weights\n\u001b[0;32m    302\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_intercept:\n\u001b[1;32m--> 303\u001b[0m     grad[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mgrad_pointwise\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    304\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m coef\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    305\u001b[0m     grad \u001b[38;5;241m=\u001b[39m grad\u001b[38;5;241m.\u001b[39mravel(order\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mF\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\huk-cc\\Lib\\site-packages\\numpy\\core\\_methods.py:47\u001b[0m, in \u001b[0;36m_sum\u001b[1;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_amin\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     44\u001b[0m           initial\u001b[38;5;241m=\u001b[39m_NoValue, where\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m umr_minimum(a, axis, \u001b[38;5;28;01mNone\u001b[39;00m, out, keepdims, initial, where)\n\u001b[1;32m---> 47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_sum\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     48\u001b[0m          initial\u001b[38;5;241m=\u001b[39m_NoValue, where\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m umr_sum(a, axis, dtype, out, keepdims, initial, where)\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_prod\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     52\u001b[0m           initial\u001b[38;5;241m=\u001b[39m_NoValue, where\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cleaned_lr_bertembed_model = train_model(X_train_cleaned_bert, y_train_cleaned, X_test_cleaned_bert, y_test_cleaned, None, 'LogisticRegression')\n",
    "topic_merged_lr_bertembed_model = train_model(X_train_topic_merged_bert, y_train_topic_merged, X_test_topic_merged_bert, y_test_topic_merged, None, 'LogisticRegression')\n",
    "balanced_us_lr_bertembed_model = train_model(X_train_balanced_us_bert, y_train_balanced_us, X_test_balanced_us_bert, y_test_balanced_us, None, 'LogisticRegression')\n",
    "balanced_os_lr_bertembed_model = train_model(X_train_balanced_os_bert, y_train_balanced_os, X_test_balanced_os_bert, y_test_balanced_os, None, 'LogisticRegression')\n",
    "\n",
    "evaluate_model(cleaned_lr_bertembed_model, X_test_cleaned_bert, y_test_cleaned)\n",
    "evaluate_model(topic_merged_lr_bertembed_model, X_test_topic_merged_bert, y_test_topic_merged)\n",
    "evaluate_model(balanced_us_lr_bertembed_model, X_test_balanced_us_bert, y_test_balanced_us)\n",
    "evaluate_model(balanced_os_lr_bertembed_model, X_test_balanced_os_bert, y_test_balanced_os)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This does not converge, lets raise the total number of iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 849.36s\n",
      "Training time: 867.21s\n",
      "Training time: 275.43s\n",
      "Training time: 2121.90s\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Irrelevant       0.61      0.38      0.47       172\n",
      "    Negative       0.57      0.75      0.65       266\n",
      "     Neutral       0.61      0.52      0.56       285\n",
      "    Positive       0.63      0.69      0.66       277\n",
      "\n",
      "    accuracy                           0.60      1000\n",
      "   macro avg       0.61      0.58      0.58      1000\n",
      "weighted avg       0.61      0.60      0.60      1000\n",
      "\n",
      "Prediction time: 16.19s\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Irrelevant       0.57      0.40      0.47       172\n",
      "    Negative       0.61      0.74      0.67       266\n",
      "     Neutral       0.64      0.57      0.60       285\n",
      "    Positive       0.62      0.68      0.65       277\n",
      "\n",
      "    accuracy                           0.62      1000\n",
      "   macro avg       0.61      0.60      0.60      1000\n",
      "weighted avg       0.62      0.62      0.61      1000\n",
      "\n",
      "Prediction time: 0.47s\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Irrelevant       0.45      0.45      0.45       172\n",
      "    Negative       0.60      0.66      0.63       266\n",
      "     Neutral       0.54      0.48      0.50       285\n",
      "    Positive       0.59      0.59      0.59       277\n",
      "\n",
      "    accuracy                           0.55      1000\n",
      "   macro avg       0.54      0.55      0.54      1000\n",
      "weighted avg       0.55      0.55      0.55      1000\n",
      "\n",
      "Prediction time: 0.24s\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Irrelevant       0.45      0.47      0.46       172\n",
      "    Negative       0.62      0.70      0.66       266\n",
      "     Neutral       0.58      0.48      0.52       285\n",
      "    Positive       0.59      0.61      0.60       277\n",
      "\n",
      "    accuracy                           0.57      1000\n",
      "   macro avg       0.56      0.56      0.56      1000\n",
      "weighted avg       0.57      0.57      0.57      1000\n",
      "\n",
      "Prediction time: 0.72s\n"
     ]
    }
   ],
   "source": [
    "cleaned_lr_pca_bertembed_model = train_model(X_train_cleaned_bert, y_train_cleaned, X_test_cleaned_bert, y_test_cleaned, None, 'LogisticRegression', log_reg_max_iter=5000)\n",
    "topic_merged_lr_pca_bertembed_model = train_model(X_train_topic_merged_bert, y_train_topic_merged, X_test_topic_merged_bert, y_test_topic_merged, None, 'LogisticRegression', log_reg_max_iter=5000)\n",
    "balanced_us_lr_pca_bertembed_model = train_model(X_train_balanced_us_bert, y_train_balanced_us, X_test_balanced_us_bert, y_test_balanced_us, None, 'LogisticRegression', log_reg_max_iter=5000)\n",
    "balanced_os_lr_pca_bertembed_model = train_model(X_train_balanced_os_bert, y_train_balanced_os, X_test_balanced_os_bert, y_test_balanced_os, None, 'LogisticRegression', log_reg_max_iter=5000)\n",
    "\n",
    "evaluate_model(cleaned_lr_pca_bertembed_model, X_test_cleaned_bert, y_test_cleaned)\n",
    "evaluate_model(topic_merged_lr_pca_bertembed_model, X_test_topic_merged_bert, y_test_topic_merged)\n",
    "evaluate_model(balanced_us_lr_pca_bertembed_model, X_test_balanced_us_bert, y_test_balanced_us)\n",
    "evaluate_model(balanced_os_lr_pca_bertembed_model, X_test_balanced_os_bert, y_test_balanced_os)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BEST PERFORMANCE (Accuracy) = Topic Merged with score 0.62"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets save the best model\n",
    "save_model(topic_merged_lr_pca_bertembed_model, 'models/simple/distilbert_embed/topic_merged_lr_pca_bertembed_model.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DistilBERT Embeddings + Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 3.43s\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Irrelevant       0.00      0.00      0.00       172\n",
      "    Negative       0.50      0.61      0.55       266\n",
      "     Neutral       0.51      0.42      0.46       285\n",
      "    Positive       0.44      0.70      0.54       277\n",
      "\n",
      "    accuracy                           0.48      1000\n",
      "   macro avg       0.36      0.43      0.39      1000\n",
      "weighted avg       0.40      0.48      0.43      1000\n",
      "\n",
      "Prediction time: 0.06s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\envs\\huk-cc\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\ProgramData\\Anaconda3\\envs\\huk-cc\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\ProgramData\\Anaconda3\\envs\\huk-cc\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 2.94s\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Irrelevant       0.00      0.00      0.00       172\n",
      "    Negative       0.53      0.49      0.51       266\n",
      "     Neutral       0.59      0.23      0.33       285\n",
      "    Positive       0.37      0.86      0.52       277\n",
      "\n",
      "    accuracy                           0.43      1000\n",
      "   macro avg       0.37      0.39      0.34      1000\n",
      "weighted avg       0.41      0.43      0.37      1000\n",
      "\n",
      "Prediction time: 0.06s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\envs\\huk-cc\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\ProgramData\\Anaconda3\\envs\\huk-cc\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\ProgramData\\Anaconda3\\envs\\huk-cc\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 1.18s\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Irrelevant       0.00      0.00      0.00       172\n",
      "    Negative       0.41      0.89      0.56       266\n",
      "     Neutral       0.53      0.26      0.35       285\n",
      "    Positive       0.52      0.53      0.52       277\n",
      "\n",
      "    accuracy                           0.46      1000\n",
      "   macro avg       0.36      0.42      0.36      1000\n",
      "weighted avg       0.40      0.46      0.39      1000\n",
      "\n",
      "Prediction time: 0.11s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\envs\\huk-cc\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\ProgramData\\Anaconda3\\envs\\huk-cc\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\ProgramData\\Anaconda3\\envs\\huk-cc\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 3.44s\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Irrelevant       0.00      0.00      0.00       172\n",
      "    Negative       0.40      0.89      0.55       266\n",
      "     Neutral       0.61      0.22      0.32       285\n",
      "    Positive       0.49      0.53      0.51       277\n",
      "\n",
      "    accuracy                           0.45      1000\n",
      "   macro avg       0.37      0.41      0.35      1000\n",
      "weighted avg       0.42      0.45      0.38      1000\n",
      "\n",
      "Prediction time: 0.05s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\envs\\huk-cc\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\ProgramData\\Anaconda3\\envs\\huk-cc\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\ProgramData\\Anaconda3\\envs\\huk-cc\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "cleaned_mnb_bertembed_model = train_model(X_train_cleaned_bert, y_train_cleaned, X_test_cleaned_bert, y_test_cleaned, None, 'MultinomialNB')\n",
    "evaluate_model(cleaned_mnb_bertembed_model, X_test_cleaned_bert, y_test_cleaned)\n",
    "\n",
    "topic_merged_mnb_bertembed_model = train_model(X_train_topic_merged_bert, y_train_topic_merged, X_test_topic_merged_bert, y_test_topic_merged, None, 'MultinomialNB')\n",
    "evaluate_model(topic_merged_mnb_bertembed_model, X_test_topic_merged_bert, y_test_topic_merged)\n",
    "\n",
    "balanced_us_mnb_bertembed_model = train_model(X_train_balanced_us_bert, y_train_balanced_us, X_test_balanced_us_bert, y_test_balanced_us, None, 'MultinomialNB')\n",
    "evaluate_model(balanced_us_mnb_bertembed_model, X_test_balanced_us_bert, y_test_balanced_us)\n",
    "\n",
    "balanced_os_mnb_bertembed_model = train_model(X_train_balanced_os_bert, y_train_balanced_os, X_test_balanced_os_bert, y_test_balanced_os, None, 'MultinomialNB')\n",
    "evaluate_model(balanced_os_mnb_bertembed_model, X_test_balanced_os_bert, y_test_balanced_os)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BEST PERFORMANCE (Accuracy) = Cleaned with score 0.48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets save the best model\n",
    "save_model(cleaned_mnb_bertembed_model, 'models/simple/distilbert_embed/cleaned_mnb_bertembed_model.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DistilBERT Embeddings + Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 411.99s\n",
      "Training time: 421.14s\n",
      "Training time: 133.85s\n",
      "Training time: 477.78s\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Irrelevant       0.70      0.70      0.70       172\n",
      "    Negative       0.73      0.82      0.77       266\n",
      "     Neutral       0.75      0.66      0.70       285\n",
      "    Positive       0.76      0.76      0.76       277\n",
      "\n",
      "    accuracy                           0.74      1000\n",
      "   macro avg       0.74      0.74      0.73      1000\n",
      "weighted avg       0.74      0.74      0.74      1000\n",
      "\n",
      "Prediction time: 0.12s\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Irrelevant       0.64      0.65      0.65       172\n",
      "    Negative       0.72      0.85      0.78       266\n",
      "     Neutral       0.78      0.64      0.70       285\n",
      "    Positive       0.73      0.73      0.73       277\n",
      "\n",
      "    accuracy                           0.72      1000\n",
      "   macro avg       0.72      0.72      0.72      1000\n",
      "weighted avg       0.73      0.72      0.72      1000\n",
      "\n",
      "Prediction time: 0.08s\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Irrelevant       0.42      0.59      0.49       172\n",
      "    Negative       0.56      0.58      0.57       266\n",
      "     Neutral       0.57      0.48      0.52       285\n",
      "    Positive       0.58      0.52      0.55       277\n",
      "\n",
      "    accuracy                           0.54      1000\n",
      "   macro avg       0.54      0.54      0.53      1000\n",
      "weighted avg       0.55      0.54      0.54      1000\n",
      "\n",
      "Prediction time: 0.18s\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Irrelevant       0.56      0.59      0.58       172\n",
      "    Negative       0.70      0.73      0.72       266\n",
      "     Neutral       0.70      0.62      0.66       285\n",
      "    Positive       0.68      0.70      0.69       277\n",
      "\n",
      "    accuracy                           0.67      1000\n",
      "   macro avg       0.66      0.66      0.66      1000\n",
      "weighted avg       0.67      0.67      0.67      1000\n",
      "\n",
      "Prediction time: 0.31s\n"
     ]
    }
   ],
   "source": [
    "cleaned_dt_bertembed_model = train_model(X_train_cleaned_bert, y_train_cleaned, X_test_cleaned_bert, y_test_cleaned, None, 'DecisionTreeClassifier')\n",
    "topic_merged_dt_bertembed_model = train_model(X_train_topic_merged_bert, y_train_topic_merged, X_test_topic_merged_bert, y_test_topic_merged, None, 'DecisionTreeClassifier')\n",
    "balanced_us_dt_bertembed_model = train_model(X_train_balanced_us_bert, y_train_balanced_us, X_test_balanced_us_bert, y_test_balanced_us, None, 'DecisionTreeClassifier')\n",
    "balanced_os_dt_bertembed_model = train_model(X_train_balanced_os_bert, y_train_balanced_os, X_test_balanced_os_bert, y_test_balanced_os, None, 'DecisionTreeClassifier')\n",
    "\n",
    "evaluate_model(cleaned_dt_bertembed_model, X_test_cleaned_bert, y_test_cleaned)\n",
    "evaluate_model(topic_merged_dt_bertembed_model, X_test_topic_merged_bert, y_test_topic_merged)\n",
    "evaluate_model(balanced_us_dt_bertembed_model, X_test_balanced_us_bert, y_test_balanced_us)\n",
    "evaluate_model(balanced_os_dt_bertembed_model, X_test_balanced_os_bert, y_test_balanced_os)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BEST PERFORMANCE (Accuracy) = Cleaned with score 0.74"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets save the best model\n",
    "save_model(cleaned_dt_bertembed_model, 'models/simple/distilbert_embed/cleaned_dt_bertembed_model.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DistilBERT Embeddings + Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 993.55s\n",
      "Training time: 778.36s\n",
      "Training time: 273.92s\n",
      "Training time: 1057.97s\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Irrelevant       0.96      0.65      0.78       172\n",
      "    Negative       0.75      0.94      0.84       266\n",
      "     Neutral       0.82      0.74      0.78       285\n",
      "    Positive       0.82      0.86      0.84       277\n",
      "\n",
      "    accuracy                           0.81      1000\n",
      "   macro avg       0.84      0.80      0.81      1000\n",
      "weighted avg       0.82      0.81      0.81      1000\n",
      "\n",
      "Prediction time: 0.88s\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Irrelevant       0.97      0.67      0.79       172\n",
      "    Negative       0.78      0.94      0.85       266\n",
      "     Neutral       0.85      0.78      0.81       285\n",
      "    Positive       0.82      0.88      0.85       277\n",
      "\n",
      "    accuracy                           0.83      1000\n",
      "   macro avg       0.85      0.82      0.83      1000\n",
      "weighted avg       0.84      0.83      0.83      1000\n",
      "\n",
      "Prediction time: 0.51s\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Irrelevant       0.63      0.70      0.66       172\n",
      "    Negative       0.72      0.80      0.76       266\n",
      "     Neutral       0.73      0.66      0.70       285\n",
      "    Positive       0.76      0.71      0.73       277\n",
      "\n",
      "    accuracy                           0.72      1000\n",
      "   macro avg       0.71      0.72      0.71      1000\n",
      "weighted avg       0.72      0.72      0.72      1000\n",
      "\n",
      "Prediction time: 0.52s\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Irrelevant       0.93      0.66      0.77       172\n",
      "    Negative       0.78      0.92      0.85       266\n",
      "     Neutral       0.85      0.76      0.80       285\n",
      "    Positive       0.78      0.87      0.83       277\n",
      "\n",
      "    accuracy                           0.82      1000\n",
      "   macro avg       0.84      0.80      0.81      1000\n",
      "weighted avg       0.83      0.82      0.81      1000\n",
      "\n",
      "Prediction time: 0.39s\n"
     ]
    }
   ],
   "source": [
    "cleaned_rf_bertembed_model = train_model(X_train_cleaned_bert, y_train_cleaned, X_test_cleaned_bert, y_test_cleaned, None, 'RandomForestClassifier')\n",
    "topic_merged_rf_bertembed_model = train_model(X_train_topic_merged_bert, y_train_topic_merged, X_test_topic_merged_bert, y_test_topic_merged, None, 'RandomForestClassifier')\n",
    "balanced_us_rf_bertembed_model = train_model(X_train_balanced_us_bert, y_train_balanced_us, X_test_balanced_us_bert, y_test_balanced_us, None, 'RandomForestClassifier')\n",
    "balanced_os_rf_bertembed_model = train_model(X_train_balanced_os_bert, y_train_balanced_os, X_test_balanced_os_bert, y_test_balanced_os, None, 'RandomForestClassifier')\n",
    "\n",
    "evaluate_model(cleaned_rf_bertembed_model, X_test_cleaned_bert, y_test_cleaned)\n",
    "evaluate_model(topic_merged_rf_bertembed_model, X_test_topic_merged_bert, y_test_topic_merged)\n",
    "evaluate_model(balanced_us_rf_bertembed_model, X_test_balanced_us_bert, y_test_balanced_us)\n",
    "evaluate_model(balanced_os_rf_bertembed_model, X_test_balanced_os_bert, y_test_balanced_os)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BEST PERFORMANCE (Accuracy) = Topic Merged with score 0.83"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets save the best model\n",
    "save_model(topic_merged_rf_bertembed_model, 'models/simple/distilbert_embed/topic_merged_rf_bertembed_model.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DistilBERT Embeddings + kNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 0.18s\n",
      "Training time: 0.19s\n",
      "Training time: 0.09s\n",
      "Training time: 0.28s\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Irrelevant       0.85      0.84      0.85       172\n",
      "    Negative       0.81      0.93      0.87       266\n",
      "     Neutral       0.93      0.81      0.87       285\n",
      "    Positive       0.87      0.86      0.87       277\n",
      "\n",
      "    accuracy                           0.86      1000\n",
      "   macro avg       0.86      0.86      0.86      1000\n",
      "weighted avg       0.87      0.86      0.86      1000\n",
      "\n",
      "Prediction time: 4.37s\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Irrelevant       0.88      0.85      0.87       172\n",
      "    Negative       0.85      0.93      0.89       266\n",
      "     Neutral       0.93      0.86      0.90       285\n",
      "    Positive       0.88      0.89      0.88       277\n",
      "\n",
      "    accuracy                           0.89      1000\n",
      "   macro avg       0.89      0.88      0.88      1000\n",
      "weighted avg       0.89      0.89      0.89      1000\n",
      "\n",
      "Prediction time: 4.17s\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Irrelevant       0.58      0.71      0.64       172\n",
      "    Negative       0.66      0.77      0.71       266\n",
      "     Neutral       0.75      0.65      0.70       285\n",
      "    Positive       0.74      0.62      0.67       277\n",
      "\n",
      "    accuracy                           0.68      1000\n",
      "   macro avg       0.68      0.69      0.68      1000\n",
      "weighted avg       0.69      0.68      0.68      1000\n",
      "\n",
      "Prediction time: 1.73s\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Irrelevant       0.83      0.82      0.83       172\n",
      "    Negative       0.84      0.87      0.86       266\n",
      "     Neutral       0.87      0.82      0.84       285\n",
      "    Positive       0.82      0.86      0.84       277\n",
      "\n",
      "    accuracy                           0.84      1000\n",
      "   macro avg       0.84      0.84      0.84      1000\n",
      "weighted avg       0.84      0.84      0.84      1000\n",
      "\n",
      "Prediction time: 8.05s\n"
     ]
    }
   ],
   "source": [
    "cleaned_knn_bertembed_model = train_model(X_train_cleaned_bert, y_train_cleaned, X_test_cleaned_bert, y_test_cleaned, None, 'KNeighborsClassifier')\n",
    "topic_merged_knn_bertembed_model = train_model(X_train_topic_merged_bert, y_train_topic_merged, X_test_topic_merged_bert, y_test_topic_merged, None, 'KNeighborsClassifier')\n",
    "balanced_us_knn_bertembed_model = train_model(X_train_balanced_us_bert, y_train_balanced_us, X_test_balanced_us_bert, y_test_balanced_us, None, 'KNeighborsClassifier')\n",
    "balanced_os_knn_bertembed_model = train_model(X_train_balanced_os_bert, y_train_balanced_os, X_test_balanced_os_bert, y_test_balanced_os, None, 'KNeighborsClassifier')\n",
    "\n",
    "evaluate_model(cleaned_knn_bertembed_model, X_test_cleaned_bert, y_test_cleaned)\n",
    "evaluate_model(topic_merged_knn_bertembed_model, X_test_topic_merged_bert, y_test_topic_merged)\n",
    "evaluate_model(balanced_us_knn_bertembed_model, X_test_balanced_us_bert, y_test_balanced_us)\n",
    "evaluate_model(balanced_os_knn_bertembed_model, X_test_balanced_os_bert, y_test_balanced_os)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BEST PERFORMANCE (Accuracy) = Topic Merged with score 0.89"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets save the best model\n",
    "save_model(topic_merged_knn_bertembed_model, 'models/simple/distilbert_embed/topic_merged_knn_bertembed_model.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRANSFORMER MODELS (DistilBERT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we want SOTA technologies and efficient inference times in CPU, DistilBERT is a fine candidate for both of the constraints.\n",
    "\n",
    "Let us prepare the dataset class for efficient loading into the transformer model using torch and also implement the same metrics that we used in simple models for validation scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "\n",
    "# Convert to torch datasets\n",
    "class SentimentDataset(torch.utils.data.Dataset):\n",
    "    LABEL_MAP = {'Positive': 0, 'Neutral': 1, 'Negative': 2, 'Irrelevant': 3}\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = [self.LABEL_MAP[label] for label in labels]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# Define metrics\n",
    "def compute_metrics(p):\n",
    "    preds = np.argmax(p.predictions, axis=1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(p.label_ids, preds, average='weighted')\n",
    "    acc = accuracy_score(p.label_ids, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can quickly load the embeddings that we saved earlier\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "train_cleaned_encodings = torch.load('/gd/MyDrive/huk-cc/data/train_cleaned_encodings.pt')\n",
    "test_cleaned_encodings = torch.load('/gd/MyDrive/huk-cc/data/validation_cleaned_encodings.pt')\n",
    "\n",
    "train_topic_merged_encodings = torch.load('/gd/MyDrive/huk-cc/data/train_topic_merged_encodings.pt')\n",
    "test_topic_merged_encodings = torch.load('/gd/MyDrive/huk-cc/data/validation_topic_merged_encodings.pt')\n",
    "\n",
    "train_balanced_us_encodings = torch.load('/gd/MyDrive/huk-cc/data/train_balanced_us_encodings.pt')\n",
    "test_balanced_us_encodings = torch.load('/gd/MyDrive/huk-cc/data/validation_balanced_us_encodings.pt')\n",
    "\n",
    "train_balanced_os_encodings = torch.load('/gd/MyDrive/huk-cc/data/train_balanced_os_encodings.pt')\n",
    "test_balanced_os_encodings = torch.load('/gd/MyDrive/huk-cc/data/validation_balanced_os_encodings.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets create the datasets\n",
    "\n",
    "train_cleaned_dataset = SentimentDataset(train_cleaned_encodings, train_df_cleaned['sentiment'].values)\n",
    "test_cleaned_dataset = SentimentDataset(test_cleaned_encodings, test_df_cleaned['sentiment'].values)\n",
    "\n",
    "train_topic_merged_dataset = SentimentDataset(train_topic_merged_encodings, train_df_topic_merged['sentiment'].values)\n",
    "test_topic_merged_dataset = SentimentDataset(test_topic_merged_encodings, test_df_topic_merged['sentiment'].values)\n",
    "\n",
    "train_balanced_us_dataset = SentimentDataset(train_balanced_us_encodings, train_df_balanced_us['sentiment'].values)\n",
    "test_balanced_us_dataset = SentimentDataset(test_balanced_us_encodings, test_df_topic_merged['sentiment'].values)\n",
    "\n",
    "train_balanced_os_dataset = SentimentDataset(train_balanced_os_encodings, train_df_balanced_os['sentiment'].values)\n",
    "test_balanced_os_dataset = SentimentDataset(test_balanced_os_encodings, test_df_topic_merged['sentiment'].values)\n",
    "\n",
    "experiments = [\n",
    "    (\"train_cleaned\", train_cleaned_dataset, test_cleaned_dataset),\n",
    "    (\"train_topic_merged\", train_topic_merged_dataset, test_topic_merged_dataset),\n",
    "    (\"train_balanced_us\", train_balanced_us_dataset, test_balanced_us_dataset),\n",
    "    (\"train_balanced_os\", train_balanced_os_dataset, test_balanced_os_dataset)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "\n",
    "# Define custom tokens for topic merged data\n",
    "SPECIAL_TOKENS = ['[TOPIC]', '[TWEET]']\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "tokenizer.add_tokens(SPECIAL_TOKENS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on train_cleaned\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "362b93102fa9461d95d9a85cf05b9a57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3282' max='3282' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3282/3282 33:09, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.835900</td>\n",
       "      <td>0.608662</td>\n",
       "      <td>0.770000</td>\n",
       "      <td>0.765528</td>\n",
       "      <td>0.772475</td>\n",
       "      <td>0.770000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.522000</td>\n",
       "      <td>0.321145</td>\n",
       "      <td>0.903000</td>\n",
       "      <td>0.903014</td>\n",
       "      <td>0.905103</td>\n",
       "      <td>0.903000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.404700</td>\n",
       "      <td>0.244823</td>\n",
       "      <td>0.923000</td>\n",
       "      <td>0.922881</td>\n",
       "      <td>0.923359</td>\n",
       "      <td>0.923000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='16' max='16' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [16/16 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Training on train_topic_merged\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3282' max='3282' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3282/3282 31:24, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.810400</td>\n",
       "      <td>0.599950</td>\n",
       "      <td>0.784000</td>\n",
       "      <td>0.780825</td>\n",
       "      <td>0.787434</td>\n",
       "      <td>0.784000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.511100</td>\n",
       "      <td>0.311909</td>\n",
       "      <td>0.897000</td>\n",
       "      <td>0.896793</td>\n",
       "      <td>0.899347</td>\n",
       "      <td>0.897000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.361600</td>\n",
       "      <td>0.231839</td>\n",
       "      <td>0.931000</td>\n",
       "      <td>0.930925</td>\n",
       "      <td>0.931010</td>\n",
       "      <td>0.931000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='16' max='16' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [16/16 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Training on train_balanced_us\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1368' max='1368' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1368/1368 10:19, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.055700</td>\n",
       "      <td>0.962070</td>\n",
       "      <td>0.610000</td>\n",
       "      <td>0.586121</td>\n",
       "      <td>0.610405</td>\n",
       "      <td>0.610000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.846000</td>\n",
       "      <td>0.708811</td>\n",
       "      <td>0.741000</td>\n",
       "      <td>0.739903</td>\n",
       "      <td>0.741737</td>\n",
       "      <td>0.741000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.627600</td>\n",
       "      <td>0.626912</td>\n",
       "      <td>0.784000</td>\n",
       "      <td>0.783019</td>\n",
       "      <td>0.783550</td>\n",
       "      <td>0.784000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='16' max='16' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [16/16 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Training on train_balanced_os\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1562' max='5505' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1562/5505 14:46 < 37:19, 1.76 it/s, Epoch 0.85/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for experiment_name, training_dataset, test_dataset in experiments:\n",
    "    print(f\"Training on {experiment_name}\")\n",
    "    # Initialize the model\n",
    "    model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=4)\n",
    "\n",
    "    # Resize model embeddings to accommodate new tokens\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "    # Training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f'models/distilbert_ft/{experiment_name}',\n",
    "        num_train_epochs=3,\n",
    "        per_device_train_batch_size=64,\n",
    "        per_device_eval_batch_size=64,\n",
    "        warmup_steps=500,\n",
    "        learning_rate=2e-5,\n",
    "        weight_decay=0.01,\n",
    "        logging_steps=10,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "    )\n",
    "\n",
    "    # Initialize Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=training_dataset,\n",
    "        eval_dataset=test_dataset,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    # Train and evaluate the model\n",
    "    trainer.train()\n",
    "    trainer.evaluate()\n",
    "    print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on train_balanced_os\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93740c4e5bd444c39109f26bf023ae22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='233' max='5505' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 233/5505 06:04 < 2:18:47, 0.63 it/s, Epoch 0.13/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5505' max='5505' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5505/5505 2:28:07, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.424100</td>\n",
       "      <td>0.351523</td>\n",
       "      <td>0.892000</td>\n",
       "      <td>0.892111</td>\n",
       "      <td>0.894640</td>\n",
       "      <td>0.892000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.169100</td>\n",
       "      <td>0.213234</td>\n",
       "      <td>0.932000</td>\n",
       "      <td>0.932067</td>\n",
       "      <td>0.933724</td>\n",
       "      <td>0.932000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.079500</td>\n",
       "      <td>0.159309</td>\n",
       "      <td>0.962000</td>\n",
       "      <td>0.962002</td>\n",
       "      <td>0.962121</td>\n",
       "      <td>0.962000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='16' max='16' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [16/16 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# final model crashed during training... lets try again\n",
    "\n",
    "for experiment_name, training_dataset, test_dataset in experiments[3:]:\n",
    "    print(f\"Training on {experiment_name}\")\n",
    "    # Initialize the model\n",
    "    model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=4)\n",
    "\n",
    "    # Resize model embeddings to accommodate new tokens\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "    # Training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f'/gd/MyDrive/models/{experiment_name}',\n",
    "        num_train_epochs=3,\n",
    "        per_device_train_batch_size=64,\n",
    "        per_device_eval_batch_size=64,\n",
    "        warmup_steps=500,\n",
    "        learning_rate=2e-5,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir=f'/gd/MyDrive/logs/{experiment_name}',\n",
    "        logging_steps=10,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "    )\n",
    "\n",
    "    # Initialize Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=training_dataset,\n",
    "        eval_dataset=test_dataset,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    # Train and evaluate the model\n",
    "    trainer.train()\n",
    "    trainer.evaluate()\n",
    "    print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final scores among finetuned DistilBERTs (Best Accuracy)\n",
    "\n",
    "- Cleaned: 0.92\n",
    "- Topic Merged: 0.93\n",
    "- Balanced w/ Undersampling: 0.78\n",
    "- Balanced w/ Oversampling: 0.96"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODEL LEADERBOARD\n",
    "\n",
    "For overview, here's a list of all saved models ranked by the accuracy score and then by training time.\n",
    "\n",
    "| Model | Dataset | Embeddings | Training Time | Accuracy |\n",
    "|----------|----------|----------|----------|----------|\n",
    "| kNN | Topic Merged | TF-IDF | 0.98 | 7.40s |\n",
    "| Random Forest | Topic Merged | TF-IDF | 0.98 | 1724.63s |\n",
    "| Transformer (DistilBERT) | Balanced via Oversampling | DistilBERT Embeddings | 0.96 | 8887s |\n",
    "| Decision Tree | Cleaned | TF-IDF | 0.93 | 132.02s |\n",
    "| Transformer (DistilBERT) | Topic Merged | DistilBERT Embeddings | 0.93 | 1884s |\n",
    "| Transformer (DistilBERT) | Cleaned | DistilBERT Embeddings | 0.92 | 1989s |\n",
    "| kNN | Topic Merged | DistilBERT Embeddings | 0.89 | 0.19s |\n",
    "| Logistic Regression | Cleaned | TF-IDF | 0.89 | 103.96s |\n",
    "| Random Forest | Topic Merged | DistilBERT Embeddings | 0.83 | 778.36s |\n",
    "| Multinomial Naive Bayes | Balanced via Oversampling | TF-IDF | 0.81 | 7.95s |\n",
    "| Transformer (DistilBERT) | Balanced via Undersampling | DistilBERT Embeddings | 0.78 | 619s |\n",
    "| Decision Tree | Cleaned | DistilBERT Embeddings | 0.74 | 411.99s |\n",
    "| Logistic Regression | Topic Merged | DistilBERT Embeddings | 0.62 | 867.21s |\n",
    "| Multinomial Naive Bayes | Cleaned | DistilBERT Embeddings | 0.48 | 3.43s |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RESULTS\n",
    "\n",
    "- Transformer is not the best model!\n",
    "    - However one may argue that simple models overfit too much and are not able to generalize, perhaps the validation data is too similar to training data.\n",
    "    - All transformer models, except undersampled, have >0.9 accuracy.\n",
    "- kNN (k=5) is both fast and accurate and the best model.\n",
    "- Merging topic into training data seems to boost performance.\n",
    "- Balanced representation with oversampling is beneficial for transformers, but not that much for simple models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "huk-cc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
