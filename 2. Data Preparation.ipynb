{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA PREPARATION\n",
    "\n",
    "With the findings from data exploration, we can create our datasets. Since we will be using both standard and SOTA models, we will prepare datasets according to them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# lets load the cleaned data, which will be the basis for all types of datasets we will create\n",
    "train_df_cleaned = pd.read_csv('data/training_cleaned.csv')\n",
    "test_df_cleaned = pd.read_csv('data/validation_cleaned.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Including topic information\n",
    "\n",
    "We simply prepend the topic to the tweet and obtain a new column which we will use further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_topic_merged = train_df_cleaned.copy()\n",
    "train_df_topic_merged['topic_tweet'] = train_df_topic_merged['topic'] + ' ' + train_df_topic_merged['tweet']\n",
    "\n",
    "test_df_topic_merged = test_df_cleaned.copy()\n",
    "test_df_topic_merged['topic_tweet'] = test_df_topic_merged['topic'] + ' ' + test_df_topic_merged['tweet']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_topic_merged.to_csv('data/training_topic_merged.csv', index=False)\n",
    "test_df_topic_merged.to_csv('data/validation_topic_merged.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Making a balanced dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# balance the dataset within each topic by undersampling the larger classes\n",
    "\n",
    "from sklearn.utils import resample\n",
    "\n",
    "train_df_balanced_us = pd.DataFrame(columns=train_df_topic_merged.columns)\n",
    "train_df_balanced_os = pd.DataFrame(columns=train_df_topic_merged.columns)\n",
    "\n",
    "topics = train_df_topic_merged['topic'].unique()\n",
    "\n",
    "for topic in topics:\n",
    "    topic_df = train_df_topic_merged[train_df_topic_merged['topic'] == topic]\n",
    "\n",
    "    # for undersampling, we will resample larger classes to the size of the smallest class\n",
    "    min_class_size = topic_df['sentiment'].value_counts().min()\n",
    "    resampled_df = pd.DataFrame(columns=topic_df.columns)\n",
    "\n",
    "    for sentiment in topic_df['sentiment'].unique():\n",
    "        sentiment_df = topic_df[topic_df['sentiment'] == sentiment]\n",
    "        resampled_df = pd.concat([resampled_df, resample(sentiment_df, replace=False, n_samples=min_class_size, random_state=123)])\n",
    "\n",
    "    train_df_balanced_us = pd.concat([train_df_balanced_us, resampled_df])\n",
    "\n",
    "    # for oversampling, we will resample smaller classes to the size of the largest class\n",
    "    max_class_size = topic_df['sentiment'].value_counts().max()\n",
    "    resampled_df = pd.DataFrame(columns=topic_df.columns)\n",
    "\n",
    "    for sentiment in topic_df['sentiment'].unique():\n",
    "        sentiment_df = topic_df[topic_df['sentiment'] == sentiment]\n",
    "        resampled_df = pd.concat([resampled_df, resample(sentiment_df, replace=True, n_samples=max_class_size, random_state=123)])\n",
    "    train_df_balanced_os = pd.concat([train_df_balanced_os, resampled_df])\n",
    "\n",
    "train_df_balanced_us.to_csv('data/training_balanced_us.csv', index=False)\n",
    "train_df_balanced_os.to_csv('data/training_balanced_os.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets create partitions\n",
    "\n",
    "# 1. Cleaned data\n",
    "X_train_cleaned, y_train_cleaned = train_df_cleaned['tweet'], train_df_cleaned['sentiment']\n",
    "X_test_cleaned, y_test_cleaned = test_df_cleaned['tweet'], test_df_cleaned['sentiment']\n",
    "\n",
    "# 2. Topic merged data\n",
    "X_train_topic_merged, y_train_topic_merged = train_df_topic_merged['topic_tweet'], train_df_topic_merged['sentiment']\n",
    "X_test_topic_merged, y_test_topic_merged = test_df_topic_merged['topic_tweet'], test_df_topic_merged['sentiment']\n",
    "\n",
    "# 3. Balanced undersampled data\n",
    "X_train_balanced_us, y_train_balanced_us = train_df_balanced_us['topic_tweet'], train_df_balanced_us['sentiment']\n",
    "X_test_balanced_us, y_test_balanced_us = test_df_topic_merged['topic_tweet'], test_df_topic_merged['sentiment']\n",
    "\n",
    "# 4. Balanced oversampled data\n",
    "X_train_balanced_os, y_train_balanced_os = train_df_balanced_os['topic_tweet'], train_df_balanced_os['sentiment']\n",
    "X_test_balanced_os, y_test_balanced_os = test_df_topic_merged['topic_tweet'], test_df_topic_merged['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let us embed the data using pretrained DistilBERT model\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "def get_bert_embeddings(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", add_special_tokens=True)\n",
    "    outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state[:, 0, :]  # cls token\n",
    "\n",
    "def get_bert_representations(X):\n",
    "    return torch.cat([get_bert_embeddings(text).detach().cpu() for text in tqdm(X)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets save these models in order to avoid recomputing them\n",
    "\n",
    "X_train_cleaned_bert = get_bert_representations(X_train_cleaned)\n",
    "X_test_cleaned_bert = get_bert_representations(X_test_cleaned)\n",
    "torch.save(X_train_cleaned_bert, 'data/X_train_cleaned_bert.pt')\n",
    "torch.save(X_test_cleaned_bert, 'data/X_test_cleaned_bert.pt')\n",
    "\n",
    "X_train_topic_merged_bert = get_bert_representations(X_train_topic_merged)\n",
    "X_test_topic_merged_bert = get_bert_representations(X_test_topic_merged)\n",
    "torch.save(X_train_topic_merged_bert, 'data/X_train_topic_merged_bert.pt')\n",
    "torch.save(X_test_topic_merged_bert, 'data/X_test_topic_merged_bert.pt')\n",
    "\n",
    "# balanced datasets will used the X_test_topic_merged_bert as the test set\n",
    "\n",
    "X_train_balanced_us_bert = get_bert_representations(X_train_balanced_us)\n",
    "torch.save(X_train_balanced_us_bert, 'data/X_train_balanced_us_bert.pt')\n",
    "\n",
    "X_train_balanced_os_bert = get_bert_representations(X_train_balanced_os)\n",
    "torch.save(X_train_balanced_os_bert, 'data/X_train_balanced_os_bert.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "huk-cc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
